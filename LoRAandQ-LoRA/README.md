# ğŸ“š LoRA & QLoRA: Parameter-Efficient Fine-Tuning for Large Language Models

> **TL;DR:** Fine-tune huge language models affordably by training tiny adapter matrices instead of all parameters. **LoRA** does this on full-precision models. **QLoRA** adds **4-bit quantization** for maximum memory efficiency â€” without sacrificing performance.

---

## ğŸ¯ What Problem Does This Solve?

Fine-tuning large language models (LLMs) like **LLaMA** or **GPT** traditionally requires:

* ğŸš€ **Massive GPU memory** (100 GB+ for 7B models)
* ğŸ•’ **Days/weeks of training time**
* ğŸ’° **Expensive hardware infrastructure**
* ğŸ“¦ **Multiple full model copies** for each fine-tuned version

**LoRA** and **QLoRA** make this process **affordable, fast, and memory-efficient** by fine-tuning just a tiny fraction of parameters.

---

## ğŸ”§ What is LoRA (Low-Rank Adaptation)?

### ğŸ“ Concept

Instead of updating full weight matrices during fine-tuning:

1. **Freeze pretrained model weights** `W`
2. **Introduce small trainable matrices** `A` and `B`
3. **Train only these adapters**
4. Combine them at inference:

$$
W_{\text{new}} = W + A \times B
$$

---

### ğŸ“ Mathematical Foundation

For a weight matrix $W \in \mathbb{R}^{d \times d}$:

* Updates are decomposed as: $\Delta W = A \times B$
* Where:

  * $A \in \mathbb{R}^{d \times r}$
  * $B \in \mathbb{R}^{r \times d}$
* $r \ll d$ (typically 4, 8, 16)

**Why "Low-Rank"?**
Because the product $A \times B$ has rank â‰¤ $r$, forcing efficient, generalizable adaptations.

---

### ğŸ’¾ Memory Benefits

* 7B model (FP16) â†’ **\~13GB**
* LoRA adapters â†’ **\~10â€“50MB**
* Total trainable params: **<1% of the model**

---

## âš¡ What is QLoRA?

### ğŸš€ Core Enhancement

**QLoRA = LoRA + 4-bit quantization of base model**

### ğŸ” Process

1. Quantize pretrained model to **4-bit NF4 (Normalized Float 4-bit)**
2. Add LoRA adapters to attention layers
3. Train adapters in **16-bit precision**
4. Keep base model frozen at **4-bit**

### ğŸ“‰ Memory Savings

| Model | FP16    | 4-bit NF4 |
| :---- | :------ | :-------- |
| 7B    | \~13GB  | \~3.5GB   |
| 13B   | \~24GB  | \~6.5GB   |
| 70B   | \~120GB | \~35GB    |

Yes â€” a **70B model can fine-tune on a single A100** using QLoRA.

---

## ğŸ—ï¸ Where LoRA is Applied

Typically inserted into:

* `q_proj` (query projection)
* `v_proj` (value projection)
* `k_proj` (key projection)
* `o_proj` (output projection)
* Gate/Up projections in MLP layers

---

## ğŸ“ Key Hyperparameters

| Hyperparameter   | Description                       | Typical Values |
| :--------------- | :-------------------------------- | :------------- |
| `rank (r)`       | LoRA adapter dimension            | 4â€“64           |
| `alpha`          | Scaling factor (usually `2Ã—rank`) | 16â€“128         |
| `dropout`        | Regularization                    | 0.05â€“0.1       |
| `target_modules` | Which layers to adapt             | See above      |

**Example LoRA Config:**

```python
from peft import LoraConfig

lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM"
)
```

---

## ğŸ“Š LoRA vs QLoRA vs Full Fine-tuning

| Method           | Memory (7B) | Training Time | Performance | Flexibility |
| :--------------- | :---------- | :------------ | :---------- | :---------- |
| Full Fine-tuning | \~130GB     | Days          | 100%        | Low         |
| LoRA             | \~15GB      | Hours         | 95â€“99%      | High        |
| QLoRA            | \~9GB       | Hours         | 95â€“99%      | High        |

---

## ğŸ› ï¸ Libraries & Tools

* `peft` (Hugging Face): LoRA / QLoRA adapters
* `bitsandbytes`: 4-bit quantization backends
* `transformers`: Pretrained model hub
* `trl`: Training utilities

**Installation:**

```bash
pip install torch transformers peft bitsandbytes trl
```

---

## ğŸ¯ Use Cases

* Task-specific fine-tuning (chat, summarization, coding)
* Domain adaptation (medical, legal, finance)
* Instruction-tuning and alignment
* Multiple adapters for a single base model

**When to pick:**

* Use **LoRA** for <13B models on 24GB+ GPUs
* Use **QLoRA** for 13B+ models or <24GB setups

---

## ğŸ” Technical Deep Dive

### Quantization Formats:

* `FP16`: 16-bit floating point
* `INT8`: 8-bit integer
* `NF4`: 4-bit normalized float (used in QLoRA)

### Training Flow:

* Forward pass: Quantized base + LoRA adapters
* Backward pass: Only adapters updated
* Optimizer updates: Only LoRA params

### Inference Modes:

* Merged (apply `W + AB` directly)
* Separate (switchable adapters)
* Multi-adapter stacking

---

## ğŸ“ˆ Best Practices

* Start with `rank=16`, `alpha=32`
* LoRA Dropout: 0.05â€“0.1
* High-quality, clean, balanced data
* Track perplexity and overfitting
* Use gradient accumulation for memory limits

---

## ğŸš€ Advanced Techniques

* **Multi-LoRA**: Load multiple adapters dynamically
* **DoRA**: Decomposed LoRA for even higher efficiency
* **AdaLoRA**: Dynamically adjust adapter rank per layer

---

## ğŸ›¡ï¸ Common Pitfalls

* Too high learning rate â†’ instability
* Low rank â†’ underfitting
* Missing target modules â†’ no effect
* Long sequences + small VRAM â†’ OOM errors

---

## ğŸ“š Resources

* ğŸ“„ [LoRA Paper (2021)](https://arxiv.org/abs/2106.09685)
* ğŸ“„ [QLoRA Paper (2023)](https://arxiv.org/abs/2305.14314)
* ğŸ“– [Hugging Face PEFT Docs](https://huggingface.co/docs/peft/index)
* ğŸ“ [BitsAndBytes Repo](https://github.com/TimDettmers/bitsandbytes)

---

## ğŸ”„ Quick Commands

```bash
# Install dependencies
pip install torch transformers peft bitsandbytes trl

# Fine-tune with LoRA or QLoRA
python train.py --model_name "meta-llama/Llama-2-7b-hf" \
                --dataset "your_dataset" \
                --lora_rank 16 \
                --quantization 4bit

# Merge LoRA adapters with base model for inference
python merge_lora.py --base_model "model_path" \
                     --lora_weights "lora_path" \
                     --output_path "merged_model"
```

---

## ğŸ’¡ Key Takeaways

âœ… LoRA trains <1% of parameters
âœ… QLoRA uses 4-bit models for major memory savings
âœ… Comparable performance to full fine-tuning
âœ… Fine-tune 13Bâ€“70B models on a single GPU
âœ… Extremely flexible, efficient, and production-friendly

---

> **For AI/LLM developers**: LoRA and QLoRA are now essential tools for anyone serious about practical large model fine-tuning without enterprise hardware.

