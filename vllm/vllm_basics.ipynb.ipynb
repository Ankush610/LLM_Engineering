{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dab00454",
   "metadata": {},
   "source": [
    "# **vLLM: High-Performance LLM Serving**\n",
    "\n",
    "## **What is vLLM?**\n",
    "\n",
    "vLLM is an open-source library designed for high-throughput and memory-efficient serving of Large Language Models (LLMs). It implements optimized memory management techniques to maximize inference speed while minimizing resource usage.\n",
    "\n",
    "## **Key Features**\n",
    "\n",
    "- **PagedAttention**: Innovative memory management technique that outperforms traditional implementations\n",
    "- **Continuous batching**: Dynamically processes requests without waiting for batch completion\n",
    "- **Tensor parallelism**: Distributes model weights across multiple GPUs\n",
    "- **Quantization support**: Runs models in lower precision formats (INT8, FP16, etc.)\n",
    "- **OpenAI-compatible API**: Drop-in replacement for OpenAI's API\n",
    "\n",
    "## **Performance Benefits**\n",
    "\n",
    "- Up to 24x higher throughput compared to standard implementations\n",
    "- Significantly reduced latency for concurrent requests\n",
    "- Efficient memory usage enabling larger context lengths\n",
    "- Seamless scaling across multiple GPUs\n",
    "\n",
    "## **Supported Architectures**\n",
    "\n",
    "- Supports most popular model families:\n",
    "  - Llama, Llama 2, Mistral, CodeLlama\n",
    "  - Mixtral, Falcon, MPT, Gemma\n",
    "  - Phi, Qwen, BLOOM, and more\n",
    "\n",
    "## **Integration Options**\n",
    "\n",
    "- **Python API**: Direct integration into Python applications\n",
    "- **REST API**: OpenAI-compatible endpoint for language-agnostic use\n",
    "- **Framework integrations**: Works with LangChain, LlamaIndex, etc.\n",
    "\n",
    "\n",
    "## **vLLM can work with distributed GPUs in several ways:**\n",
    "\n",
    "### **Tensor Parallelism:**\n",
    "\n",
    "  - Splits model weights across multiple GPUs on a single machine\n",
    "  - Configured using the --tensor-parallel-size parameter\n",
    "  - Each layer's computation is distributed across GPUs\n",
    "\n",
    "\n",
    "### **Pipeline Parallelism:**\n",
    "\n",
    "  - Splits model layers across different GPUs\n",
    "  - Different from tensor parallelism which splits individual layers\n",
    "  - Useful for extremely large models that don't fit on a single GPU even with tensor parallelism\n",
    "\n",
    "\n",
    "### **Multi-Node Distributed Inference:**\n",
    "\n",
    "  - vLLM supports distributing models across multiple machines\n",
    "  - Uses Ray as the backend for distributed computing\n",
    "  - Can combine both tensor and pipeline parallelism across nodes\n",
    "\n",
    "## **Usage Example**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c966dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting vLLM server\n",
    "# python -m vllm.entrypoints.openai.api_server --model mistralai/Mistral-7B-Instruct-v0.2\n",
    "\n",
    "# Using with OpenAI client\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"EMPTY\")\n",
    "response = client.chat.completions.create(\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66313ed",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "source": [
    "## **When to Use vLLM**\n",
    "\n",
    "- Serving LLMs in production environments\n",
    "- Running local models with near cloud-service performance\n",
    "- Building applications that require high throughput\n",
    "- Handling concurrent user requests efficiently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d0d196",
   "metadata": {},
   "source": [
    "# **Running Local LLMs with vLLM and OpenAI API Compatibility**\n",
    "\n",
    "This notebook documents the process of running local large language models using vLLM with OpenAI API compatibility.\n",
    "\n",
    "## **Setup Process**\n",
    "\n",
    "### 1. **Installing vLLM**\n",
    "\n",
    "\n",
    "First, ensure vLLM is installed with the appropriate CUDA version for your system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eb1f96",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "! pip install vllm\n",
    "! pip install \"vllm[triton]\" --extra-index-url https://download.pytorch.org/whl/cu124 <-- user your cuda version\n",
    "# Or with specific CUDA version\n",
    "# pip install vllm-with-cuda11x  # Example for CUDA 11.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a043961",
   "metadata": {},
   "source": [
    "### 2. **Starting the vLLM Server**\n",
    "\n",
    "Launch the vLLM server with the following command to utilize multiple GPUs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d61ae1",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "!python -m vllm.entrypoints.openai.api_server \\\n",
    "  --model /path/to/model/snapshots/snapshotID \\\n",
    "  --tensor-parallel-size 2 \\  # Adjust based on number of GPUs\n",
    "  --gpu-memory-utilization 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca80b938",
   "metadata": {},
   "source": [
    "Parameters explained:\n",
    "- `--model`: Path to the model snapshot\n",
    "- `--tensor-parallel-size`: Number of GPUs to use in parallel\n",
    "- `--gpu-memory-utilization`: Portion of GPU memory to utilize (0.9 = 90%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd61174",
   "metadata": {},
   "source": [
    "### 3. **Verifying Available Models**\n",
    "\n",
    "Check which models are available on your server:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f32f32",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "!curl http://localhost:8000/v1/models # This returns the available model path under the \"root\" parameter, which you'll need for API calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d230fd",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "source": [
    "## **Using the Model with OpenAI API**\n",
    "\n",
    "### **Basic Chat Completion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4c8139",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize client with local server\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"EMPTY\"  # Required parameter even if not used\n",
    ")\n",
    "\n",
    "# Make API call\n",
    "response = client.chat.completions.create(\n",
    "    model=\"path/to/model/snapshots/<snapshotID>\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What's the capital of France?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Extract and print response\n",
    "print(response.choices[0].message.content)\n",
    "# Note: Ensure that the model path and snapshot ID are correctly specified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5784c4f4",
   "metadata": {},
   "source": [
    "### **With LangChain Integration**\n",
    "\n",
    "When using with LangChain, you can connect as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d325570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_773129/2817957409.py:4: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  gpu_llm = ChatOpenAI(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=' The capital of France is Paris. It is located in the north-central part of the country. Paris is known for its iconic landmarks such as the Eiffel Tower, Louvre Museum, Notre-Dame Cathedral, and the Arc de Triomphe. It is also a major cultural, artistic, and fashion center. The city has a population of over 2 million people, and the wider metropolitan area has a population of over 12 million people, making it one of the most populous urban areas in Europe. The city of Paris was founded in the 3rd century BC by a Celtic people called the Parisii, and it became the capital of the Kingdom of France in the 10th century. It has been the capital of France ever since, with the exception of a brief period during the French Revolution when the seat of government was moved to the city of Tours.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 190, 'prompt_tokens': 11, 'total_tokens': 201, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': '/home/omjadhav/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/e0bc86c23ce5aae1db576c8cca6f06f1f73af2db', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-def42b34-2aa3-405c-be37-4a67cdcceba7-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Initialize the LLM\n",
    "gpu_llm = ChatOpenAI(\n",
    "    model_name=\"/path/to/model/snapshots/snapshotID\",\n",
    "    openai_api_base=\"http://localhost:8000/v1\",\n",
    "    openai_api_key=\"EMPTY\"\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "response = gpu_llm.invoke(\"What's the capital of France?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0316f3c6",
   "metadata": {},
   "source": [
    "## **Best Practices**\n",
    "\n",
    "1. **Memory Management**: Monitor GPU memory usage with tools like `nvidia-smi`\n",
    "2. **Batch Size Tuning**: Adjust batch size for optimal throughput if processing multiple requests\n",
    "3. **Quantization**: Consider quantized models (like 4-bit or 8-bit) for larger models with limited GPU memory\n",
    "4. **Caching**: Enable response caching for repetitive queries\n",
    "\n",
    "## **Troubleshooting**\n",
    "\n",
    "- If experiencing CUDA out-of-memory errors, reduce `--gpu-memory-utilization` or use a smaller model\n",
    "- For tensor parallelism issues, ensure all GPUs are of the same architecture\n",
    "- If the model loads but inference is slow, consider adjusting KV cache settings\n",
    "\n",
    "## **References**\n",
    "\n",
    "- [vLLM GitHub Repository](https://github.com/vllm-project/vllm)\n",
    "- [Documentation](https://docs.vllm.ai/)\n",
    "- [PagedAttention Paper](https://arxiv.org/abs/2309.06180)\n",
    "- [OpenAI API Compatibility Guide](https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html)\n",
    "- [LangChain Integration Documentation](https://python.langchain.com/docs/integrations/llms/vllm)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resumeAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
