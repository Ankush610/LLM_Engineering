{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr \n",
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Gradio-Overview**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def louder(sent):\n",
    "    print(\"This will print word in upper case\")\n",
    "    return sent.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7866\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7866/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gradio code \n",
    "gr.Interface(fn=louder,inputs='textbox',outputs='textbox').launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\interface.py:399: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7867\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gradio code \n",
    "gr.Interface(fn=louder,inputs='textbox',outputs='textbox',allow_flagging='never').launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\interface.py:399: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7868\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view = gr.Interface(\n",
    "    fn=louder,\n",
    "    inputs=[gr.Textbox(label=\"input\")],\n",
    "    outputs=[gr.Textbox(label='output')],\n",
    "    allow_flagging='never'\n",
    ")\n",
    "\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\interface.py:399: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7875\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7875/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view = gr.Interface(\n",
    "    fn=louder,\n",
    "    inputs=[gr.Textbox(label=\"input\",lines=3)],\n",
    "    outputs=[gr.Textbox(label='output',lines=1)],\n",
    "    allow_flagging='never'\n",
    ")\n",
    "\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Gradio with Ollama**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from IPython.display import display, Markdown ,update_display\n",
    "\n",
    "def call_ollama(user_prompt):\n",
    "    response = ollama.chat(model='llama3.2', messages=[\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': \"You are a helpful AI chatbot\"\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': user_prompt\n",
    "        }\n",
    "    ])\n",
    "\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\interface.py:399: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7871\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7871/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view = gr.Interface(\n",
    "    fn=call_ollama,\n",
    "    inputs=[gr.Textbox(label=\"input\")],\n",
    "    outputs=[gr.Textbox(label='output')],\n",
    "    allow_flagging='never'\n",
    ")\n",
    "\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\interface.py:399: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7879\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7879/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view = gr.Interface(\n",
    "    fn=call_ollama,\n",
    "    inputs=[gr.Textbox(label=\"input\")],\n",
    "    outputs=[gr.Markdown(label='output')],\n",
    "    allow_flagging='never'\n",
    ")\n",
    "\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Gradio with Ollama - Streaming Response**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import gradio as gr\n",
    "from IPython.display import display, Markdown ,update_display\n",
    "import time\n",
    "\n",
    "def stream_ollama(user_prompt):\n",
    "\n",
    "    yield \"ðŸ•’ Processing your request...\" \n",
    "\n",
    "    stream = ollama.chat(model='llama3.2', messages=[\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': \"You are a helpful AI chatbot\"\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': user_prompt\n",
    "        }\n",
    "    ],stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk['message']['content']\n",
    "        response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use Gradio Blocks for streaming\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        input_box = gr.Textbox(label=\"Input\", placeholder=\"Type your message...\")\n",
    "    with gr.Row():\n",
    "        output_box = gr.Markdown(label=\"Output\")\n",
    "    with gr.Row():\n",
    "        submit_button = gr.Button(\"Submit\")\n",
    "\n",
    "    # Connect button to the streaming function\n",
    "    submit_button.click(fn=stream_ollama, inputs=input_box, outputs=output_box)\n",
    "\n",
    "# Launch the Gradio app\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Gradio with ChatGPT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import gradio as gr\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key is found and loaded\n"
     ]
    }
   ],
   "source": [
    "load_dotenv() # we're saving openai api key as env. variable here \n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not api_key:\n",
    "    print(\"No API Key found !\")\n",
    "elif not api_key.startswith(\"sk-proj-\"):\n",
    "    print(\"Not a OpenAI API key\")\n",
    "elif api_key.strip() != api_key:\n",
    "    print(\"Remove blank spaces From API key\")\n",
    "else:\n",
    "    print('API key is found and loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt(user_prompt):\n",
    "\n",
    "    response = openai.chat.completions.create(\n",
    "        model = 'gpt-4o-mini',\n",
    "        messages = [\n",
    "        {'role':'system', 'content':\"You are a Helpful AI Assistant named ChatGPT\"},\n",
    "        {'role':'user', 'content':user_prompt}\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\interface.py:399: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7896\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7896/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.Interface(fn=call_gpt,\n",
    "             inputs='textbox',\n",
    "             outputs='textbox',\n",
    "             allow_flagging='never').launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\interface.py:399: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7898\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7898/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.Interface(fn=call_gpt,\n",
    "             inputs=[gr.Textbox(label='input',lines=3)],\n",
    "             outputs=[gr.Markdown(label='Response')],\n",
    "             allow_flagging='never').launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Gradio with ChatGPT - Streaming Response**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_gpt(user_prompt):\n",
    "\n",
    "    yield \"ðŸ•‘ Generating Response... \"\n",
    "\n",
    "    stream = openai.chat.completions.create(\n",
    "        model = 'gpt-4o-mini',\n",
    "        messages = [\n",
    "        {'role':'system', 'content':\"You are a Helpful AI Assistant named ChatGPT\"},\n",
    "        {'role':'user', 'content':user_prompt}\n",
    "    ],\n",
    "    stream=True\n",
    "    )\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\interface.py:399: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7900\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7900/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.Interface(fn=stream_gpt,\n",
    "             inputs=[gr.Textbox(label='input',lines=3)],\n",
    "             outputs=[gr.Markdown(label='Response')],\n",
    "             allow_flagging='never').launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ollama ChatBOT-UI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'user: hello my name is abhishek assistant: Hello Abhishek! How can I assist you today? user: how many languages do you know  assistant: I can understand and generate text in multiple languages, including but not limited to English, Spanish, French, German, Chinese, Hindi, Arabic, and many more. If you have a specific language in mind, feel free to ask! user: kya hal hai bhai  assistant: Main theek hoon, Shukriya! Aap kaise hain? Kya main aapki kisi tarah se madad kar sakta hoon?'"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Given history data (This is what gradio returns ), we can create a formatted sting which can passed as history to our ollama model \n",
    "history = [\n",
    "    {'role': 'user', 'metadata': {'title': None}, 'content': 'hello my name is abhishek', 'options': None},\n",
    "    {'role': 'assistant', 'metadata': {'title': None}, 'content': 'Hello Abhishek! How can I assist you today?', 'options': None},\n",
    "    {'role': 'user', 'metadata': {'title': None}, 'content': 'how many languages do you know ', 'options': None},\n",
    "    {'role': 'assistant', 'metadata': {'title': None}, 'content': 'I can understand and generate text in multiple languages, including but not limited to English, Spanish, French, German, Chinese, Hindi, Arabic, and many more. If you have a specific language in mind, feel free to ask!', 'options': None},\n",
    "    {'role': 'user', 'metadata': {'title': None}, 'content': 'kya hal hai bhai ', 'options': None},\n",
    "    {'role': 'assistant', 'metadata': {'title': None}, 'content': 'Main theek hoon, Shukriya! Aap kaise hain? Kya main aapki kisi tarah se madad kar sakta hoon?', 'options': None},\n",
    "]\n",
    "\n",
    "# Construct the desired string\n",
    "formatted_history = \" \".join(f\"{entry['role']}: {entry['content']}\" for entry in history)\n",
    "\n",
    "formatted_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import ollama\n",
    "import time \n",
    "\n",
    "def chat_ollama(message, history):\n",
    "    \n",
    "    # Extract only the last five entries from the history and format them\n",
    "    last_five = history[-5:]\n",
    "\n",
    "    formatted_history = \" \".join(f\"{entry['role']}: {entry['content']}\" for entry in last_five)\n",
    "\n",
    "    stream = ollama.chat(\n",
    "        model='llama3.2',\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': \"You are an intelligent and helpful assistant. Answer the current question based solely on the knowledge you have and the context from the conversation history if it is relevant. Do not explicitly mention or reference the history unless explicitly asked to recall past interactions.\"},\n",
    "            {'role': 'user', 'content': f\"\"\"Conversation History :{formatted_history} \\nQuestion: {message}\"\"\"},\n",
    "        ],\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk['message']['content']\n",
    "        yield response\n",
    "\n",
    "\n",
    "gr.ChatInterface(fn=chat_ollama,type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ChatGPT ChatBOT-UI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message,history):\n",
    "\n",
    "    yield \"ðŸ•‘ Generating Response... \"\n",
    "\n",
    "    messages = [{'role':'system','content':\"You are a helpful AI assistant\"}]\n",
    "\n",
    "    for user_message , assistant_message in history:\n",
    "        messages.append({'role':'user','content':user_message})\n",
    "        messages.append({'role':'user','content':assistant_message})\n",
    "\n",
    "    messages.append({'role':'user','content':message})\n",
    "\n",
    "    stream = openai.chat.completions.create(\n",
    "        model = 'gpt-4o-mini',\n",
    "        messages = messages,\n",
    "    stream=True\n",
    "    )\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\components\\chatbot.py:243: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n",
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:7950/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: HEAD http://127.0.0.1:7950/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7950/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Note:** \n",
    "For Ollama model when using **\"gr.ChatInterface(fn=chat,type=\"messages\").launch()\"** here we used **type=\"messages\"** but while using GPT we used this **\"gr.ChatInterface(fn=chat).launch()\"** only the difference is in the **output(History)** in they produce:\n",
    "\n",
    "- **With type=\"messages\" :**\n",
    "    - history is like this : \n",
    "    ```python \n",
    "    [{'role': 'user', 'metadata': {'title': None}, 'content': 'hello my name is abhishek', 'options': None}, {'role': 'assistant', 'metadata': {'title': None}, 'content': 'Hello Abhishek! How can I assist you today?', 'options': None}]\n",
    "    ```\n",
    "- **Without type=\"messages\" :**\n",
    "    - history is like this : \n",
    "    ```python\n",
    "    [['hello my name is abhishek','Hello Abhishek! How can I assist you today?'],[...]]\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
