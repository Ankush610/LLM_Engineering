{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import ollama\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv() # we're saving openai api key as env. variable here \n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not api_key:\n",
    "    print(\"No API Key found !\")\n",
    "elif not api_key.startswith(\"sk-proj-\"):\n",
    "    print(\"Not a OpenAI API key\")\n",
    "elif api_key.strip() != api_key:\n",
    "    print(\"Remove blank spaces From API key\")\n",
    "else:\n",
    "    print('API key is found and loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_ollama(message, history):\n",
    "\n",
    "    formatted_history = []\n",
    "\n",
    "    for user_message, assistant_message in history:\n",
    "        formatted_history.append({'role': 'user', 'content': user_message})\n",
    "        formatted_history.append({'role': 'assistant', 'content': assistant_message})\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model='llama3.2',\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': \"You are an assistant who always lies. Answer the current question based solely on the knowledge you have and the context from the conversation history if it is relevant. Do not explicitly mention or reference the history unless explicitly asked to recall past interactions.\"},\n",
    "            {'role': 'user', 'content': f\"Conversation History: {str(formatted_history)}\\nQuestion: {message}\"},\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response['message']['content']\n",
    "\n",
    "\n",
    "def chat_gpt(message, history):\n",
    "    messages = [{'role': 'system', 'content': \"You are someone who corrects everyone's lies.\"}]\n",
    "\n",
    "    for user_message, assistant_message in history:\n",
    "        messages.append({'role': 'user', 'content': user_message})\n",
    "        messages.append({'role': 'assistant', 'content': assistant_message})\n",
    "\n",
    "    messages.append({'role': 'user', 'content': message})\n",
    "\n",
    "    response = openai.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=messages,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "    \n",
    "def battle():\n",
    "    ollama_response = \"earth is flat\"\n",
    "    gpt_history = []\n",
    "    ollama_history = []\n",
    "\n",
    "    while True:\n",
    "        # GPT responds to Ollama's message\n",
    "        gpt_response = chat_gpt(ollama_response, gpt_history)\n",
    "\n",
    "        # Ollama responds to GPT's message\n",
    "        ollama_response = chat_ollama(gpt_response, ollama_history)\n",
    "\n",
    "        # Update histories\n",
    "        gpt_history.append((ollama_response, gpt_response))\n",
    "        ollama_history.append((gpt_response, ollama_response))\n",
    "\n",
    "        # Print the messages\n",
    "        print(\"GPT Message:\", gpt_response)\n",
    "        print('\\n')\n",
    "        print(\"Ollama Message:\", ollama_response)\n",
    "        print('\\n')\n",
    "        \n",
    "        # Break condition for demonstration (infinite loop avoided)\n",
    "        if len(gpt_history) >= 4:  # Limit to 10 interactions\n",
    "            break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    battle() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
