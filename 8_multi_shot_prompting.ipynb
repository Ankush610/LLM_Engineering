{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import ollama\n",
    "import time \n",
    "\n",
    "def chat_ollama(message, history):\n",
    "    \n",
    "    # Extract only the last five entries from the history and format them\n",
    "    last_five = history[-5:]\n",
    "\n",
    "    formatted_history = \" \".join(f\"{entry['role']}: {entry['content']}\" for entry in last_five)\n",
    "\n",
    "    stream = ollama.chat(\n",
    "        model='llama3.2',\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': \"You are an intelligent and helpful assistant. Answer the current question based solely on the knowledge you have and the context from the conversation history if it is relevant. Do not explicitly mention or reference the history unless explicitly asked to recall past interactions.\"},\n",
    "            {'role': 'user', 'content': f\"\"\"Conversation History :{formatted_history} \\nQuestion: {message}\"\"\"},\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream['message']['content']:\n",
    "        response += chunk or ''\n",
    "        yield response\n",
    "        time.sleep(0.009) # to make chat generate slowly \n",
    "\n",
    "\n",
    "gr.ChatInterface(fn=chat_ollama, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_gpt(message,history):\n",
    "\n",
    "    messages = [{'role':'system','content':\"You are a helpful AI assistant\"}]\n",
    "\n",
    "    for user_message , assistant_message in history:\n",
    "        messages.append({'role':'user','content':user_message})\n",
    "        messages.append({'role':'user','content':assistant_message})\n",
    "\n",
    "    messages.append({'role':'user','content':message})\n",
    "\n",
    "    stream = openai.chat.completions.create(\n",
    "        model = 'gpt-4o-mini',\n",
    "        messages = messages,\n",
    "    stream=True\n",
    "    )\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.3",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
