{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown,display,update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key is found and loaded\n"
     ]
    }
   ],
   "source": [
    "load_dotenv() # we're saving openai api key as env. variable here \n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not api_key:\n",
    "    print(\"No API Key found !\")\n",
    "elif not api_key.startswith(\"sk-proj-\"):\n",
    "    print(\"Not a OpenAI API key\")\n",
    "elif api_key.strip() != api_key:\n",
    "    print(\"Remove blank spaces From API key\")\n",
    "else:\n",
    "    print('API key is found and loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt(user_msg):\n",
    "\n",
    "    system_msg = \"be a helpful AI_assisant\"\n",
    "    \n",
    "\n",
    "    stream = openai.chat.completions.create(\n",
    "        \n",
    "        model='gpt-4o-mini',\n",
    "\n",
    "        messages=[\n",
    "            {'role':'system','content':system_msg},\n",
    "            {'role':'user','content':user_msg}\n",
    "        ],\n",
    "\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "        update_display(Markdown(response), display_id=display_handle.display_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Message : \n",
      "GPT Response : \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Hello! How can I assist you today?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# THis is a basic structure here we're not sliding a context window as we progressively ask more questions \n",
    "\n",
    "user_msg = \"\"\n",
    "temp_msg = \"temp\"\n",
    "\n",
    "while temp_msg:\n",
    "    temp_msg = input(\"Enter User Input : \")\n",
    "    user_msg += temp_msg\n",
    "    print(\"User Message : \"+temp_msg)\n",
    "    print(\"GPT Response : \")\n",
    "    gpt_response = call_gpt(user_msg)\n",
    "    user_msg += str(gpt_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Convo-History Manual Logic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_conversation(messages):\n",
    "    formatted = \"\"\n",
    "    for i, (user_msg, gpt_msg) in enumerate(messages, 1):\n",
    "        formatted += f\"Previous Q{i}: {user_msg}\\n\"\n",
    "        formatted += f\"Previous A{i}: {gpt_msg}\\n\"\n",
    "    return formatted\n",
    "\n",
    "def call_gpt_with_context(current_msg, conversation_history):\n",
    "    # Format previous conversations\n",
    "    context = format_conversation(conversation_history)\n",
    "    \n",
    "    # Create prompt with context and current question\n",
    "    full_prompt = f\"{context}\\nCurrent Question: {current_msg}\"\n",
    "    \n",
    "    # Call GPT with the full context\n",
    "    response = call_gpt(full_prompt)\n",
    "    return response\n",
    "\n",
    "def main():\n",
    "    conversation_history = []  # List to store (question, answer) pairs\n",
    "    user_msg = \"\"\n",
    "    temp_msg = \"temp\"\n",
    "    \n",
    "    while temp_msg:\n",
    "        temp_msg = input(\"Enter User Input: \")\n",
    "        if not temp_msg:\n",
    "            break\n",
    "            \n",
    "        print(\"User Message: \" + temp_msg)\n",
    "        print(\"GPT Response: \")\n",
    "        \n",
    "        # Get response with context\n",
    "        gpt_response = call_gpt_with_context(temp_msg, conversation_history)\n",
    "        print(gpt_response)\n",
    "        \n",
    "        # Store this Q&A pair in history\n",
    "        conversation_history.append((temp_msg, gpt_response))\n",
    "        \n",
    "        # Optional: Keep only last N conversations (e.g., last 5)\n",
    "        if len(conversation_history) > 5:\n",
    "            conversation_history = conversation_history[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Message: what is my name \n",
      "GPT Response: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "I'm sorry, but I don't have access to personal information about users unless it has been shared with me in our conversation. If you'd like me to address you by a particular name, feel free to let me know!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "User Message: my name is ankush \n",
      "GPT Response: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Nice to meet you, Ankush! How can I assist you today?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "User Message: tell me about cdac \n",
      "GPT Response: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "CDAC, or the Centre for Development of Advanced Computing, is an autonomous organization in India that was established in 1988 by the Ministry of Electronics and Information Technology. It aims to develop cutting-edge technologies and promote their applications in various sectors, including software development, hardware design, and advanced computing.\n",
       "\n",
       "Key aspects of CDAC include:\n",
       "\n",
       "1. **Research and Development**: CDAC focuses on research in areas like high-performance computing, grid computing, cloud computing, artificial intelligence, and data analytics.\n",
       "\n",
       "2. **Educational Programs**: CDAC runs various academic and training programs, including post-graduate diploma courses in advanced computing, software engineering, and embedded systems.\n",
       "\n",
       "3. **Technology Transfer**: CDAC works closely with industries, government agencies, and other organizations to transfer technology and provide solutions tailored to specific needs.\n",
       "\n",
       "4. **International Collaboration**: CDAC often collaborates with international research organizations and academic institutions to enhance its capabilities and develop global partnerships.\n",
       "\n",
       "5. **Products and Solutions**: It has developed several products and platforms in areas like software, networking, and hardware solutions, addressing needs in both public and private sectors.\n",
       "\n",
       "Overall, CDAC plays a significant role in advancing computing technology in India and contributing to the country's technological growth. If you have specific aspects of CDAC you're interested in, feel free to ask!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "User Message: what is my name \n",
      "GPT Response: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Your name is Ankush."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "User Message: tell me about spiderman\n",
      "GPT Response: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Spider-Man is a fictional superhero created by writer Stan Lee and artist Steve Ditko. He first appeared in \"Amazing Fantasy\" #15 in 1962, published by Marvel Comics. The character's real name is Peter Parker, a teenager from New York City who gains superpowers after being bitten by a genetically altered spider.\n",
       "\n",
       "Peter Parker possesses several powers, including superhuman strength, agility, and the ability to cling to walls. He also has a \"spider-sense\" that alerts him to danger. He uses his intellect to invent web-shooters, devices that allow him to shoot webs and swing through the city.\n",
       "\n",
       "The character's story often explores themes of responsibility, particularly encapsulated in the famous phrase, \"With great power comes great responsibility,\" which was imparted to him by his Uncle Ben, who he feels responsible for losing due to a failure to act in time.\n",
       "\n",
       "Spider-Man has been featured in numerous comic book series, television shows, and movies, becoming one of the most iconic superheroes in popular culture. His adventures often involve battles against various villains, with notable ones including the Green Goblin, Doctor Octopus, and Venom, while also dealing with personal struggles and relationships."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "User Message: who is iron man\n",
      "GPT Response: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Iron Man is a fictional superhero appearing in American comic books published by Marvel Comics. The character was created by writer Stan Lee, developed by scripter Larry Lieber, and designed by artists Don Heck and Jack Kirby. Iron Man's real identity is Tony Stark, a wealthy industrialist and genius inventor who builds a powered suit of armor to save his life and eventually uses it to protect the world. He is a founding member of the Avengers and is known for his wit, intelligence, and resourcefulness. The character has been portrayed by Robert Downey Jr. in the Marvel Cinematic Universe, contributing significantly to Iron Man's popularity in modern culture."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "User Message: was spiderman an avenger\n",
      "GPT Response: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Yes, Spider-Man has been a member of the Avengers in the comics and the Marvel Cinematic Universe (MCU). He officially joined the Avengers in the comics during the \"Avengers\" series and has appeared alongside them in various storylines. In the MCU, he becomes part of the team in \"Captain America: Civil War\" and continues to interact with other Avengers in later films."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "User Message: tell me my name \n",
      "GPT Response: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "I’m sorry, but I don’t have access to your name or any personal information. If you’d like to share your name or any other detail, feel free to do so!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Chat-GPT Context Loop Using Specific GPT Structure of User , System , Assistant**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Message :  hi my name is ankush\n",
      "GPT Response : \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Hi Ankush! How can I assist you today?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Message :  what is ollama\n",
      "GPT Response : \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Ollama is a platform that allows users to run and manage large language models locally on their machines. It simplifies the process of downloading, running, and using these models by providing a command-line interface and containerized solutions. This means developers and researchers can experiment with and deploy AI models without needing extensive cloud infrastructure.\n",
       "\n",
       "If you have specific questions about Ollama or its functionalities, feel free to ask!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Message :  \n",
      "GPT Response : \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "It seems like your message didn’t come through. How can I assist you further?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import openai\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def call_gpt2(messages):\n",
    "    # Initialize with system message\n",
    "    conversation = [\n",
    "        {'role': 'system', 'content': \"be a helpful AI_assistant\"}\n",
    "    ]\n",
    "    \n",
    "    # Add all previous messages to conversation\n",
    "    conversation.extend(messages)\n",
    "    \n",
    "    stream = openai.chat.completions.create(\n",
    "        model='gpt-4o-mini',  # Make sure this is the correct model name\n",
    "        messages=conversation,\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    \n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            response += chunk.choices[0].delta.content\n",
    "            response_clean = response.replace(\"```\", \"\").replace(\"markdown\", \"\")\n",
    "            display_handle.update(Markdown(response_clean))\n",
    "    \n",
    "    return response\n",
    "\n",
    "def chat_loop():\n",
    "    # Store the conversation history as a list of message dictionaries\n",
    "    conversation_history = []\n",
    "    \n",
    "    while True:\n",
    "        user_msg = input(\"Enter Message (or 'quit' to exit): \")\n",
    "        if user_msg.lower() == 'quit':\n",
    "            break\n",
    "            \n",
    "        # Add user message to history\n",
    "        conversation_history.append({\n",
    "            'role': 'user',\n",
    "            'content': user_msg\n",
    "        })\n",
    "        \n",
    "        print(\"User Message : \",user_msg)\n",
    "        \n",
    "        # Get GPT response\n",
    "        print(\"GPT Response : \")\n",
    "        gpt_response = call_gpt2(conversation_history)\n",
    "        \n",
    "        # Add GPT response to history\n",
    "        conversation_history.append({\n",
    "            'role': 'assistant',\n",
    "            'content': gpt_response\n",
    "        })\n",
    "        \n",
    "        # Optional: Keep only last N messages for context window\n",
    "        max_context = 10  # Adjust as needed\n",
    "        if len(conversation_history) > max_context * 2:  # *2 because each exchange has 2 messages\n",
    "            conversation_history = conversation_history[-max_context * 2:]\n",
    "\n",
    "# Start the chat\n",
    "chat_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
