{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown,display,update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv() # we're saving openai api key as env. variable here \n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not api_key:\n",
    "    print(\"No API Key found !\")\n",
    "elif not api_key.startswith(\"sk-proj-\"):\n",
    "    print(\"Not a OpenAI API key\")\n",
    "elif api_key.strip() != api_key:\n",
    "    print(\"Remove blank spaces From API key\")\n",
    "else:\n",
    "    print('API key is found and loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt(user_msg):\n",
    "\n",
    "    system_msg = \"be a helpful AI_assisant\"\n",
    "    \n",
    "\n",
    "    stream = openai.chat.completions.create(\n",
    "        \n",
    "        model='gpt-4o-mini',\n",
    "\n",
    "        messages=[\n",
    "            {'role':'system','content':system_msg},\n",
    "            {'role':'user','content':user_msg}\n",
    "        ],\n",
    "\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "        update_display(Markdown(response), display_id=display_handle.display_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THis is a basic structure here we're not sliding a context window as we progressively ask more questions \n",
    "\n",
    "user_msg = \"\"\n",
    "temp_msg = \"temp\"\n",
    "\n",
    "while temp_msg:\n",
    "    temp_msg = input(\"Enter User Input : \")\n",
    "    user_msg += temp_msg\n",
    "    print(\"User Message : \"+temp_msg)\n",
    "    print(\"GPT Response : \")\n",
    "    gpt_response = call_gpt(user_msg)\n",
    "    user_msg += str(gpt_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Convo-History Ollama**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from IPython.display import Markdown,display,update_display\n",
    "\n",
    "def chat_ollama(message, history):\n",
    "\n",
    "    formatted_history = []\n",
    "\n",
    "    for user_message, assistant_message in history:\n",
    "        formatted_history.append({'role': 'user', 'content': user_message})\n",
    "        formatted_history.append({'role': 'assistant', 'content': assistant_message})\n",
    "\n",
    "    stream = ollama.chat(\n",
    "        model='llama3.2',\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': \"You are an assistant who always lies. Answer the current question based solely on the knowledge you have and the context from the conversation history if it is relevant. Do not explicitly mention or reference the history unless explicitly asked to recall past interactions.\"},\n",
    "            {'role': 'user', 'content': f\"Conversation History: {str(formatted_history)}\\nQuestion: {message}\"},\n",
    "        ],\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"),display_id=True)\n",
    "    for chunk in stream:\n",
    "        response += chunk['message']['content'] or ''\n",
    "        response = response.replace(\"```\", \"\").replace(\"markdown\", \"\")\n",
    "        update_display(response,display_id=display_handle.display_id)\n",
    "\n",
    "def main():\n",
    "    history = []\n",
    "    user = ''\n",
    "    \n",
    "    while user != 'exit': \n",
    "        user = input(\"Enter Question : \") \n",
    "        print(\"User Message :\",user)\n",
    "        print(\"Model response : \") \n",
    "        response = chat_ollama(user,history)\n",
    "        history.append((user,response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Conversation history with Chat-GPT**\n",
    " Context Loop Using Specific GPT Structure of User , System , Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def call_gpt2(messages):\n",
    "    # Initialize with system message\n",
    "    conversation = [\n",
    "        {'role': 'system', 'content': \"be a helpful AI_assistant\"}\n",
    "    ]\n",
    "    \n",
    "    # Add all previous messages to conversation\n",
    "    conversation.extend(messages)\n",
    "    \n",
    "    stream = openai.chat.completions.create(\n",
    "        model='gpt-4o-mini',  # Make sure this is the correct model name\n",
    "        messages=conversation,\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    \n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            response += chunk.choices[0].delta.content\n",
    "            response_clean = response.replace(\"```\", \"\").replace(\"markdown\", \"\")\n",
    "            display_handle.update(Markdown(response_clean))\n",
    "    \n",
    "    return response\n",
    "\n",
    "def chat_loop():\n",
    "    # Store the conversation history as a list of message dictionaries\n",
    "    conversation_history = []\n",
    "    \n",
    "    while True:\n",
    "        user_msg = input(\"Enter Message (or 'quit' to exit): \")\n",
    "        if user_msg.lower() == 'quit':\n",
    "            break\n",
    "            \n",
    "        # Add user message to history\n",
    "        conversation_history.append({\n",
    "            'role': 'user',\n",
    "            'content': user_msg\n",
    "        })\n",
    "        \n",
    "        print(\"User Message : \",user_msg)\n",
    "        \n",
    "        # Get GPT response\n",
    "        print(\"GPT Response : \")\n",
    "        gpt_response = call_gpt2(conversation_history)\n",
    "        \n",
    "        # Add GPT response to history\n",
    "        conversation_history.append({\n",
    "            'role': 'assistant',\n",
    "            'content': gpt_response\n",
    "        })\n",
    "        \n",
    "        # Optional: Keep only last N messages for context window\n",
    "        max_context = 10  # Adjust as needed\n",
    "        if len(conversation_history) > max_context * 2:  # *2 because each exchange has 2 messages\n",
    "            conversation_history = conversation_history[-max_context * 2:]\n",
    "\n",
    "# Start the chat\n",
    "chat_loop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
