{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Running Jupyter Code with Ollama Server**\n",
    "\n",
    "1. **Download and Install Ollama**: Ensure that Ollama is downloaded and installed on your system.\n",
    "\n",
    "2. **Check Ollama Version**:\n",
    "   ```bash\n",
    "   ollama --version\n",
    "   ```\n",
    "\n",
    "3. **Run Llama 3.2 Model**:\n",
    "   To load the Llama 3.2 model:\n",
    "   ```bash\n",
    "   ollama run llama3.2\n",
    "   ```\n",
    "\n",
    "4. **Start Ollama Server**:\n",
    "   To start the Ollama server, run:\n",
    "   ```bash\n",
    "   ollama serve\n",
    "   ```\n",
    "\n",
    "**Note**: If you face any issues while running the server, checking the background tasks using the port can help resolve conflicts. Always ensure the port is free before starting the server.\n",
    "\n",
    "1. **Check if Ollama Server is Running**:\n",
    "   Check if the server is running and using port 11434:\n",
    "   ```bash\n",
    "   netstat -ano | findstr :11434\n",
    "   ```\n",
    "\n",
    "2. **Identify and Kill Background Task (if needed)**:\n",
    "   If you see that another task is using the port, you can identify and kill it:\n",
    "   ```bash\n",
    "   tasklist /FI \"PID eq <PID>\"\n",
    "   taskkill /PID <PID> /F\n",
    "   ```\n",
    "\n",
    "3. **Close Ollama from Windows Task Bar (Optional)**:\n",
    "   If the server is still not responding or you see issues, you can use the **up arrow key** (^) on the Windows task bar (bottom-left) to close any running Ollama instances.\n",
    "\n",
    "4. **Start Jupyter Notebook**:\n",
    "   Once the server is running, start your Jupyter notebook with the necessary code to interact with Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from PyPDF2 import PdfReader\n",
    "from IPython.display import display, Markdown ,update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''You are an agent which analyses the resume and understands the context of resume and \n",
    "    extract the following fields and format the output:\n",
    "    - Name\n",
    "    - Email\n",
    "    - Phone 1\n",
    "    - Phone 2\n",
    "    - Address\n",
    "    - City\n",
    "    - LinkedIn\n",
    "    - Professional Experience (in years)\n",
    "    - Highest Education\n",
    "    - Is Fresher (yes/no)\n",
    "    - Is Student (yes/no)\n",
    "    - Skills (comma-separated)\n",
    "    - Applied For Profile\n",
    "    - Education (Institute Name, Year of Passing, Score)\n",
    "    - Projects (Project Name, Description)\n",
    "    - Professional Experience (Organisation Name, Duration, Profile)\n",
    "\n",
    "    Ensure to format the output as:\n",
    "    Name: [Value]\n",
    "    Email: [Value]\n",
    "    Phone 1: [Value]\n",
    "    ... and provide this summary of resume in markdown format'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as file:\n",
    "            reader = PdfReader(file)\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text()  # Extract text from each page\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from PDF: {e}\")\n",
    "    return text\n",
    "    \n",
    "user_prompt = extract_text_from_pdf('./data/python-developer-resume-example.pdf') + \"Provide a summary of this resume\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Resume Summary\n",
       "\n",
       "#### Contact Information\n",
       "##### Name: Giulia González\n",
       "##### Email: <gonzalez@email.com>\n",
       "##### Phone 1: (123) 456-7890\n",
       "##### Phone 2: N/A\n",
       "##### Address: Detroit, MI\n",
       "##### City: Detroit, MI\n",
       "##### LinkedIn: N/A\n",
       "##### Professional Profile Link: Github\n",
       "\n",
       "#### Education\n",
       "\n",
       "* **M.S. in Computer Science**, University of Chicago (2014 - 2016)\n",
       "\t+ Score: N/A\n",
       "* **B.S. in Computer Science**, University of Pittsburgh (2010 - 2014)\n",
       "\n",
       "#### Skills\n",
       "HTML/CSS, SQL (PostgreSQL, Oracle), JavaScript (Angular), Python (Django), REST APIs (GraphQL), AWS (Redshift, S3), Git\n",
       "\n",
       "#### Work Experience\n",
       "\n",
       "*   **Python Developer**, DoorDash (September 2017 - Present)\n",
       "    *   Duration: 6 years\n",
       "    *   Profile:\n",
       "        *   Built new Angular components for the customer-facing web app.\n",
       "        *   Collaborated with an agile team to prioritize and scope feature requests.\n",
       "        *   Reduced customer complaints by 23%.\n",
       "        *   Boosted revenue by 6% through data pipelines discovery.\n",
       "*   **Python Developer Intern**, Knewton (April 2016 - April 2017)\n",
       "    *   Duration: 1 year\n",
       "    *   Profile:\n",
       "        *   Implemented RESTful APIs in Django for internal analytics team reporting speed increase.\n",
       "        *   Reduced bugs reported by the client by 11% month-over-month.\n",
       "        *   Provided project updates and design recommendations.\n",
       "\n",
       "#### Projects\n",
       "\n",
       "*   **Cryptocurrency Price Tracker**\n",
       "    *   Creator\n",
       "    *   Description:\n",
       "        *   Incorporated API calls to several applications for efficient data storage in PostgreSQL backend.\n",
       "        *   Utilized D3.js for user dynamic visualization of price movements over time."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def call_ollama(system_prompt, user_prompt):\n",
    "    url = \"http://localhost:11434/api/chat\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": \"llama3.2\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_prompt\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, json=payload)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        full_response = \"\"\n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                json_response = line.decode('utf-8')\n",
    "                try:\n",
    "                    response_data = json.loads(json_response)\n",
    "                    if 'message' in response_data:\n",
    "                        full_response += response_data['message'].get('content', '')\n",
    "                except json.JSONDecodeError:\n",
    "                    print(\"Error decoding JSON\")\n",
    "        \n",
    "        return display(Markdown(full_response))\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "response = call_ollama(system_prompt, user_prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **OLD CODE**\n",
    "\n",
    "**NOTE :** Just Kept the old code to show how Ollama provdes answers it sends one word at a time in a chunk type response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Response: {\"model\":\"llama3.2\",\"created_at\":\"2024-12-18T10:37:04.3412715Z\",\"message\":{\"role\":\"assistant\",\"content\":\"The\"},\"done\":false}\n",
      "{\"model\":\"llama3.2\",\"created_at\":\"2024-12-18T10:37:04.5390128Z\",\"message\":{\"role\":\"assistant\",\"content\":\" capital\"},\"done\":false}\n",
      "{\"model\":\"llama3.2\",\"created_at\":\"2024-12-18T10:37:04.7603018Z\",\"message\":{\"role\":\"assistant\",\"content\":\" of\"},\"done\":false}\n",
      "{\"model\":\"llama3.2\",\"created_at\":\"2024-12-18T10:37:04.9268798Z\",\"message\":{\"role\":\"assistant\",\"content\":\" France\"},\"done\":false}\n",
      "{\"model\":\"llama3.2\",\"created_at\":\"2024-12-18T10:37:05.1936493Z\",\"message\":{\"role\":\"assistant\",\"content\":\" is\"},\"done\":false}\n",
      "{\"model\":\"llama3.2\",\"created_at\":\"2024-12-18T10:37:05.3531851Z\",\"message\":{\"role\":\"assistant\",\"content\":\" Paris\"},\"done\":false}\n",
      "{\"model\":\"llama3.2\",\"created_at\":\"2024-12-18T10:37:05.5912375Z\",\"message\":{\"role\":\"assistant\",\"content\":\".\"},\"done\":false}\n",
      "{\"model\":\"llama3.2\",\"created_at\":\"2024-12-18T10:37:05.8094624Z\",\"message\":{\"role\":\"assistant\",\"content\":\"\"},\"done_reason\":\"stop\",\"done\":true,\"total_duration\":2487808100,\"load_duration\":16617900,\"prompt_eval_count\":38,\"prompt_eval_duration\":999726000,\"eval_count\":8,\"eval_duration\":1468329000}\n",
      "\n",
      "Error parsing JSON: Extra data: line 2 column 1 (char 125)\n",
      "Response text may not be valid JSON.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:11434/api/chat\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Define the conversation prompt and specify the model\n",
    "data = {\n",
    "    \"model\": \"llama3.2\",  # Specify the model name here\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Send the POST request to the Ollama server\n",
    "response = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "# Print the raw response text for debugging\n",
    "print(\"Raw Response:\", response.text)\n",
    "\n",
    "# Try to parse the response only if it's in JSON format\n",
    "try:\n",
    "    response_json = response.json()  # Try parsing the response as JSON\n",
    "    print(\"Model Response:\", response_json)\n",
    "except ValueError as e:\n",
    "    print(\"Error parsing JSON:\", e)\n",
    "    print(\"Response text may not be valid JSON.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Easier Way**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Resume Summary\n",
       "#### Contact Information\n",
       "=====================================\n",
       "\n",
       "*   **Name:** G I U L I A\n",
       "*   **Email:** `ggonzalez@email.com`\n",
       "*   **Phone 1:** `(123) 456-7890`\n",
       "*   **LinkedIn:** Available\n",
       "*   **Phone 2:** Not specified\n",
       "*   **Address:** Detroit, MI\n",
       "\n",
       "#### Professional Experience\n",
       "==================================\n",
       "\n",
       "*   **Python Developer**, DoorDash (September 2017 - Current)\n",
       "    *   Duration: 5 years and 6 months\n",
       "    *   Location: Detroit, MI\n",
       "    *   Summary:\n",
       "        \n",
       "Python Developer\n",
       "DoorDash\n",
       "September 2017 - current Detroit, MI\n",
       "·Worked on building new Angular components for the customer-facing web app, which improved the time on page for the average user by 2 minutes\n",
       "·Collaborated with an agile team of 6, and helped prioritize and scope feature requests to ensure that the biggest impact features were worked on ﬁrst\n",
       "·Built extensive test coverage for all new features, which reduced the number of customer complaints by 23%\n",
       "·Acquired and ingested data to build and maintain data pipelines that led to discovering an opportunity for a new site feature, boosting revenue by 6%\n",
       "·Communicated with internal teams and stakeholders, working to determine solutions for the user experience\n",
       "\n",
       "\n",
       "*   **Python Developer Intern**, Knewton (April 2016 - April 2017)\n",
       "    *   Duration: 1 year\n",
       "    *   Location: Chicago, IL\n",
       "    *   Summary:\n",
       "        \n",
       "Python Developer Intern\n",
       "Knewton\n",
       "April 2016 - April 2017 Chicago, IL\n",
       "·Worked alongside another developer to implement RESTful APIs in Django that enabled internal analytics team to increase reporting speed by 24%\n",
       "·Using Selenium, built out a unit testing infrastructure for a client web application that reduced the number of bugs reported by the client by 11% month over month\n",
       "·Provided project updates to leadership team of 3, and offered recommendations for design\n",
       "·Diagnosed issues causing slow speeds in applications, and documented the process to making the database query system more robust\n",
       "·Participated in writing scalable code with a team of 4 interns and 1 developer for applications for a math course\n",
       "\n",
       "\n",
       "#### Education\n",
       "================\n",
       "\n",
       "*   **M.S. Computer Science**, University of Chicago (2014 - 2016)\n",
       "    *   Location: Chicago, IL\n",
       "    *   Score: Not specified\n",
       "*   **B.S. Computer Science**, University of Pittsburgh (2010 - 2014)\n",
       "    *   Location: Pittsburgh, PA\n",
       "    *   Score: Not specified\n",
       "\n",
       "#### Skills\n",
       "=============\n",
       "\n",
       "*   HTML/ CSS\n",
       "*   SQL (PostgreSQL, Oracle)\n",
       "*   JavaScript (Angular)\n",
       "*   Python (Django)\n",
       "*   REST APIs (GraphQL)\n",
       "*   AWS (Redshift, S3)\n",
       "*   Git"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "def call_ollama(system_prompt, user_prompt):\n",
    "    stream = ollama.chat(model='llama3.2', messages=[\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': system_prompt\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': user_prompt\n",
    "        }\n",
    "    ])\n",
    "\n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in stream['message']['content']:\n",
    "        response += chunk or''\n",
    "        response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "        update_display(Markdown(response), display_id=display_handle.display_id)\n",
    "        \n",
    "# # Example usage\n",
    "# system_prompt = \"You are a helpful assistant who explains complex topics in simple terms.\"\n",
    "# user_prompt = \"What is 2+2\"\n",
    "\n",
    "response = call_ollama(system_prompt, user_prompt)\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
