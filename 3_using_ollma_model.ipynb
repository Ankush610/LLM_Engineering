{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Running Jupyter Code with Ollama Server**\n",
    "\n",
    "1. **Download and Install Ollama**: Ensure that Ollama is downloaded and installed on your system.\n",
    "\n",
    "2. **Check Ollama Version**:\n",
    "   ```bash\n",
    "   ollama --version\n",
    "   ```\n",
    "\n",
    "3. **Run Llama 3.2 Model**:\n",
    "   To load the Llama 3.2 model:\n",
    "   ```bash\n",
    "   ollama run llama3.2\n",
    "   ```\n",
    "\n",
    "4. **Start Ollama Server**:\n",
    "   To start the Ollama server, run:\n",
    "   ```bash\n",
    "   ollama serve\n",
    "   ```\n",
    "\n",
    "**Note**: If you face any issues while running the server, checking the background tasks using the port can help resolve conflicts. Always ensure the port is free before starting the server.\n",
    "\n",
    "1. **Check if Ollama Server is Running**:\n",
    "   Check if the server is running and using port 11434:\n",
    "   ```bash\n",
    "   netstat -ano | findstr :11434\n",
    "   ```\n",
    "\n",
    "2. **Identify and Kill Background Task (if needed)**:\n",
    "   If you see that another task is using the port, you can identify and kill it:\n",
    "   ```bash\n",
    "   tasklist /FI \"PID eq <PID>\"\n",
    "   taskkill /PID <PID> /F\n",
    "   ```\n",
    "\n",
    "3. **Close Ollama from Windows Task Bar (Optional)**:\n",
    "   If the server is still not responding or you see issues, you can use the **up arrow key** (^) on the Windows task bar (bottom-left) to close any running Ollama instances.\n",
    "\n",
    "4. **Start Jupyter Notebook**:\n",
    "   Once the server is running, start your Jupyter notebook with the necessary code to interact with Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''You are an agent which analyses the resume and understands the context of resume and \n",
    "    extract the following fields and format the output:\n",
    "    - Name\n",
    "    - Email\n",
    "    - Phone 1\n",
    "    - Phone 2\n",
    "    - Address\n",
    "    - City\n",
    "    - LinkedIn\n",
    "    - Professional Experience (in years)\n",
    "    - Highest Education\n",
    "    - Is Fresher (yes/no)\n",
    "    - Is Student (yes/no)\n",
    "    - Skills (comma-separated)\n",
    "    - Applied For Profile\n",
    "    - Education (Institute Name, Year of Passing, Score)\n",
    "    - Projects (Project Name, Description)\n",
    "    - Professional Experience (Organisation Name, Duration, Profile)\n",
    "\n",
    "    Ensure to format the output as:\n",
    "    Name: [Value]\n",
    "    Email: [Value]\n",
    "    Phone 1: [Value]\n",
    "    ... and provide this summary of resume in markdown format'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as file:\n",
    "            reader = PdfReader(file)\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text()  # Extract text from each page\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from PDF: {e}\")\n",
    "    return text\n",
    "    \n",
    "user_prompt = extract_text_from_pdf('./data/python-developer-resume-example.pdf') + \"Provide a summary of this resume\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Resume Summary\n",
       "\n",
       "#### Contact Information\n",
       "- **Name:** G I U L I A\n",
       "- **Email:** ggonzalez@email.com\n",
       "- **Phone 1:** (123) 456-7890\n",
       "- **Address:** Detroit, MI\n",
       "- **City:** Detroit, MI\n",
       "- **LinkedIn:** LinkedIn\n",
       "- **Phone 2:** Not provided\n",
       "\n",
       "#### Professional Experience\n",
       "- **Professional Experience (Years):** 5 years\n",
       "- **Highest Education:** M.S. in Computer Science from University of Chicago\n",
       "- **Education (Institute Name, Year of Passing, Score):**\n",
       "  - University of Pittsburgh (2010-2014)\n",
       "  - University of Chicago (2014-2016)\n",
       "\n",
       "#### Skills\n",
       "- HTML/CSS\n",
       "- SQL (PostgreSQL, Oracle)\n",
       "- JavaScript (Angular)\n",
       "- Python (Django)\n",
       "- REST APIs (GraphQL)\n",
       "- AWS (Redshift, S3)\n",
       "- Git\n",
       "\n",
       "#### Work Experience\n",
       "- **Python Developer**, DoorDash (September 2017 - Present)\n",
       "    - Built new Angular components for the customer-facing web app, improving time on page by 2 minutes.\n",
       "    - Collaborated with an agile team to prioritize and scope feature requests, ensuring impact features were worked on first.\n",
       "    - Built extensive test coverage, reducing customer complaints by 23%.\n",
       "    - Communicated with internal teams and stakeholders, determining solutions for the user experience.\n",
       "\n",
       "- **Python Developer Intern**, Knewton (April 2016 - April 2017)\n",
       "    - Implemented RESTful APIs in Django, increasing reporting speed by 24% for the internal analytics team.\n",
       "    - Built a unit testing infrastructure using Selenium, reducing bugs reported by 11% month over month.\n",
       "    - Provided project updates to leadership and offered recommendations for design.\n",
       "\n",
       "#### Projects\n",
       "- **Cryptocurrency Price Tracker**\n",
       "    - Incorporated API calls to several applications, storing data efficiently in PostgreSQL backend.\n",
       "    - Utilized D3.js to allow users to dynamically visualize price movements over time periods of their choosing."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "url = \"http://localhost:11434/api/chat\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Define the conversation prompt and specify the model\n",
    "data = {\n",
    "    \"model\": \"llama3.2\",  # Specify the model name here\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Send the POST request to the Ollama server\n",
    "response = requests.post(url, json=data, headers=headers, stream=True)\n",
    "\n",
    "# Check if the response is successful\n",
    "if response.status_code == 200:\n",
    "    # Initialize a variable to accumulate the assistant's response\n",
    "    full_response = \"\"\n",
    "    \n",
    "    # Iterate over the chunks of data as they arrive\n",
    "    for chunk in response.iter_lines():\n",
    "        if chunk:  # Ignore empty lines\n",
    "            try:\n",
    "                # Each chunk is a part of the message, so we can parse it as JSON\n",
    "                chunk_data = json.loads(chunk.decode('utf-8'))\n",
    "                \n",
    "                # Extract the message content\n",
    "                message_content = chunk_data.get('message', {}).get('content', '')\n",
    "                full_response += message_content\n",
    "                \n",
    "                # Check if the response is done\n",
    "                if chunk_data.get('done', False):\n",
    "                    break\n",
    "                \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(\"Error parsing JSON chunk:\", e)\n",
    "    \n",
    "    # Display the model's response as Markdown\n",
    "    display(Markdown(full_response))\n",
    "else:\n",
    "    print(f\"Error {response.status_code}: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **OLD CODE** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Response: {\"model\":\"llama3.2\",\"created_at\":\"2024-12-18T09:52:40.7042198Z\",\"message\":{\"role\":\"assistant\",\"content\":\"The\"},\"done\":false}\n",
      "{\"model\":\"llama3.2\",\"created_at\":\"2024-12-18T09:52:40.8116449Z\",\"message\":{\"role\":\"assistant\",\"content\":\" capital\"},\"done\":false}\n",
      "{\"model\":\"llama3.2\",\"created_at\":\"2024-12-18T09:52:40.9195623Z\",\"message\":{\"role\":\"assistant\",\"content\":\" of\"},\"done\":false}\n",
      "{\"model\":\"llama3.2\",\"created_at\":\"2024-12-18T09:52:41.0716295Z\",\"message\":{\"role\":\"assistant\",\"content\":\" France\"},\"done\":false}\n",
      "{\"model\":\"llama3.2\",\"created_at\":\"2024-12-18T09:52:41.234114Z\",\"message\":{\"role\":\"assistant\",\"content\":\" is\"},\"done\":false}\n",
      "{\"model\":\"llama3.2\",\"created_at\":\"2024-12-18T09:52:41.3777785Z\",\"message\":{\"role\":\"assistant\",\"content\":\" Paris\"},\"done\":false}\n",
      "{\"model\":\"llama3.2\",\"created_at\":\"2024-12-18T09:52:41.4916704Z\",\"message\":{\"role\":\"assistant\",\"content\":\".\"},\"done\":false}\n",
      "{\"model\":\"llama3.2\",\"created_at\":\"2024-12-18T09:52:41.6135776Z\",\"message\":{\"role\":\"assistant\",\"content\":\"\"},\"done_reason\":\"stop\",\"done\":true,\"total_duration\":1517524900,\"load_duration\":17099400,\"prompt_eval_count\":38,\"prompt_eval_duration\":588186000,\"eval_count\":8,\"eval_duration\":908983000}\n",
      "\n",
      "Error parsing JSON: Extra data: line 2 column 1 (char 125)\n",
      "Response text may not be valid JSON.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:11434/api/chat\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Define the conversation prompt and specify the model\n",
    "data = {\n",
    "    \"model\": \"llama3.2\",  # Specify the model name here\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Send the POST request to the Ollama server\n",
    "response = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "# Print the raw response text for debugging\n",
    "print(\"Raw Response:\", response.text)\n",
    "\n",
    "# Try to parse the response only if it's in JSON format\n",
    "try:\n",
    "    response_json = response.json()  # Try parsing the response as JSON\n",
    "    print(\"Model Response:\", response_json)\n",
    "except ValueError as e:\n",
    "    print(\"Error parsing JSON:\", e)\n",
    "    print(\"Response text may not be valid JSON.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just Kept the old code to show how Ollama provdes answers it sends one word at a time in a chunk of response "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
