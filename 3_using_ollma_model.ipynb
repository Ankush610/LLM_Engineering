{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Running Jupyter Code with Ollama Server**\n",
    "\n",
    "1. **Download and Install Ollama**: Ensure that Ollama is downloaded and installed on your system.\n",
    "\n",
    "2. **Check Ollama Version**:\n",
    "   ```bash\n",
    "   ollama --version\n",
    "   ```\n",
    "\n",
    "3. **Run Llama 3.2 Model**:\n",
    "   To load the Llama 3.2 model:\n",
    "   ```bash\n",
    "   ollama run llama3.2\n",
    "   ```\n",
    "\n",
    "4. **Start Ollama Server**:\n",
    "   To start the Ollama server, run:\n",
    "   ```bash\n",
    "   ollama serve\n",
    "   ```\n",
    "\n",
    "**Note**: If you face any issues while running the server, checking the background tasks using the port can help resolve conflicts. Always ensure the port is free before starting the server.\n",
    "\n",
    "1. **Check if Ollama Server is Running**:\n",
    "   Check if the server is running and using port 11434:\n",
    "   ```bash\n",
    "   netstat -ano | findstr :11434\n",
    "   ```\n",
    "\n",
    "2. **Identify and Kill Background Task (if needed)**:\n",
    "   If you see that another task is using the port, you can identify and kill it:\n",
    "   ```bash\n",
    "   tasklist /FI \"PID eq <PID>\"\n",
    "   taskkill /PID <PID> /F\n",
    "   ```\n",
    "\n",
    "3. **Close Ollama from Windows Task Bar (Optional)**:\n",
    "   If the server is still not responding or you see issues, you can use the **up arrow key** (^) on the Windows task bar (bottom-left) to close any running Ollama instances.\n",
    "\n",
    "4. **Start Jupyter Notebook**:\n",
    "   Once the server is running, start your Jupyter notebook with the necessary code to interact with Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''You are an agent which analyses the resume and understands the context of resume and \n",
    "    extract the following fields and format the output:\n",
    "    - Name\n",
    "    - Email\n",
    "    - Phone 1\n",
    "    - Phone 2\n",
    "    - Address\n",
    "    - City\n",
    "    - LinkedIn\n",
    "    - Professional Experience (in years)\n",
    "    - Highest Education\n",
    "    - Is Fresher (yes/no)\n",
    "    - Is Student (yes/no)\n",
    "    - Skills (comma-separated)\n",
    "    - Applied For Profile\n",
    "    - Education (Institute Name, Year of Passing, Score)\n",
    "    - Projects (Project Name, Description)\n",
    "    - Professional Experience (Organisation Name, Duration, Profile)\n",
    "\n",
    "    Ensure to format the output as:\n",
    "    Name: [Value]\n",
    "    Email: [Value]\n",
    "    Phone 1: [Value]\n",
    "    ... and provide this summary of resume in markdown format'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as file:\n",
    "            reader = PdfReader(file)\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text()  # Extract text from each page\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from PDF: {e}\")\n",
    "    return text\n",
    "    \n",
    "user_prompt = extract_text_from_pdf('./data/python-developer-resume-example.pdf') + \"Provide a summary of this resume\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://localhost:11434/api/chat\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Define the conversation prompt and specify the model\n",
    "data = {\n",
    "    \"model\": \"llama3.2\",  # Specify the model name here\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Send the POST request to the Ollama server\n",
    "response = requests.post(url, json=data, headers=headers, stream=True)\n",
    "\n",
    "# Check if the response is successful\n",
    "if response.status_code == 200:\n",
    "    # Initialize a variable to accumulate the assistant's response\n",
    "    full_response = \"\"\n",
    "    \n",
    "    # Iterate over the chunks of data as they arrive\n",
    "    for chunk in response.iter_lines():\n",
    "        if chunk:  # Ignore empty lines\n",
    "            try:\n",
    "                # Each chunk is a part of the message, so we can parse it as JSON\n",
    "                chunk_data = json.loads(chunk.decode('utf-8'))\n",
    "                \n",
    "                # Extract the message content\n",
    "                message_content = chunk_data.get('message', {}).get('content', '')\n",
    "                full_response += message_content\n",
    "                \n",
    "                # Check if the response is done\n",
    "                if chunk_data.get('done', False):\n",
    "                    break\n",
    "                \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(\"Error parsing JSON chunk:\", e)\n",
    "    \n",
    "    print(\"Model Response:\", full_response)\n",
    "else:\n",
    "    print(f\"Error {response.status_code}: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Response: ### Resume Summary\n",
      "\n",
      "#### Contact Information\n",
      "| Field | Value |\n",
      "| --- | --- |\n",
      "| Name | G I U L I A G O N Z A L E Z |\n",
      "| Email | `gonzalez@email.com` |\n",
      "| Phone 1 | `(123) 456-7890` |\n",
      "| City | Detroit, MI |\n",
      "\n",
      "#### Professional Experience\n",
      "| Field | Value |\n",
      "| --- | --- |\n",
      "| Organisation Name | DoorDash |\n",
      "| Duration | September 2017 - current |\n",
      "| Profile | Python Developer |\n",
      "\n",
      "### Education\n",
      "\n",
      "| Field | Value |\n",
      "| --- | --- |\n",
      "| Institute Name | University of Chicago, University of Pittsburgh |\n",
      "| Year of Passing | 2016, 2014 |\n",
      "| Score | M.S. Computer Science (University of Chicago), B.S. Computer Science (University of Pittsburgh) |\n",
      "\n",
      "#### Skills\n",
      "`HTML/ CSS, SQL (PostgreSQL, Oracle), JavaScript (Angular), Python (Django), REST APIs (GraphQL), AWS (Redshift, S3), Git`\n",
      "\n",
      "#### Applied For Profile\n",
      "Not specified in the provided information.\n",
      "\n",
      "### Projects\n",
      "\n",
      "| Field | Value |\n",
      "| --- | --- |\n",
      "| Project Name | Cryptocurrency Price Tracker |\n",
      "| Description | Creator |\n",
      "\n",
      "#### Work Experience Details\n",
      "\n",
      "| Field | Value |\n",
      "| --- | --- |\n",
      "| Organisation Name | DoorDash, Knewton |\n",
      "| Duration | September 2017 - current, April 2016 - April 2017 |\n",
      "| Profile | Python Developer, Python Developer Intern |\n",
      "\n",
      "#### Student Status\n",
      "`Is Fresher: no`, `Is Student: no`\n",
      "\n",
      "Note that some information was not available or was not specified in the provided resume.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **OLD CODE** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Response: {\"model\":\"llama3.2\",\"created_at\":\"2024-12-18T09:48:16.4898854Z\",\"message\":{\"role\":\"assistant\",\"content\":\"The\"},\"done\":false}\n",
      "{\"model\":\"llama3.2\",\"created_at\":\"2024-12-18T09:48:16.5647504Z\",\"message\":{\"role\":\"assistant\",\"content\":\" capital\"},\"done\":false}\n",
      "{\"model\":\"llama3.2\",\"created_at\":\"2024-12-18T09:48:16.6424078Z\",\"message\":{\"role\":\"assistant\",\"content\":\" of\"},\"done\":false}\n",
      "{\"model\":\"llama3.2\",\"created_at\":\"2024-12-18T09:48:16.7355952Z\",\"message\":{\"role\":\"assistant\",\"content\":\" France\"},\"done\":false}\n",
      "{\"model\":\"llama3.2\",\"created_at\":\"2024-12-18T09:48:16.8292194Z\",\"message\":{\"role\":\"assistant\",\"content\":\" is\"},\"done\":false}\n",
      "{\"model\":\"llama3.2\",\"created_at\":\"2024-12-18T09:48:16.951255Z\",\"message\":{\"role\":\"assistant\",\"content\":\" Paris\"},\"done\":false}\n",
      "{\"model\":\"llama3.2\",\"created_at\":\"2024-12-18T09:48:17.0301931Z\",\"message\":{\"role\":\"assistant\",\"content\":\".\"},\"done\":false}\n",
      "{\"model\":\"llama3.2\",\"created_at\":\"2024-12-18T09:48:17.1088063Z\",\"message\":{\"role\":\"assistant\",\"content\":\"\"},\"done_reason\":\"stop\",\"done\":true,\"total_duration\":1171014700,\"load_duration\":28835700,\"prompt_eval_count\":38,\"prompt_eval_duration\":521084000,\"eval_count\":8,\"eval_duration\":618848000}\n",
      "\n",
      "Error parsing JSON: Extra data: line 2 column 1 (char 125)\n",
      "Response text may not be valid JSON.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:11434/api/chat\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Define the conversation prompt and specify the model\n",
    "data = {\n",
    "    \"model\": \"llama3.2\",  # Specify the model name here\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Send the POST request to the Ollama server\n",
    "response = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "# Print the raw response text for debugging\n",
    "print(\"Raw Response:\", response.text)\n",
    "\n",
    "# Try to parse the response only if it's in JSON format\n",
    "try:\n",
    "    response_json = response.json()  # Try parsing the response as JSON\n",
    "    print(\"Model Response:\", response_json)\n",
    "except ValueError as e:\n",
    "    print(\"Error parsing JSON:\", e)\n",
    "    print(\"Response text may not be valid JSON.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
