{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Running Jupyter Code with Ollama Server**\n",
    "\n",
    "1. **Download and Install Ollama**: Ensure that Ollama is downloaded and installed on your system.\n",
    "\n",
    "2. **Check Ollama Version**:\n",
    "   ```bash\n",
    "   ollama --version\n",
    "   ```\n",
    "\n",
    "3. **Run Llama 3.2 Model**:\n",
    "   To load the Llama 3.2 model:\n",
    "   ```bash\n",
    "   ollama run llama3.2\n",
    "   ```\n",
    "\n",
    "4. **Start Ollama Server**:\n",
    "   To start the Ollama server, run:\n",
    "   ```bash\n",
    "   ollama serve\n",
    "   ```\n",
    "\n",
    "**Note**: If you face any issues while running the server, checking the background tasks using the port can help resolve conflicts. Always ensure the port is free before starting the server.\n",
    "\n",
    "1. **Check if Ollama Server is Running**:\n",
    "   Check if the server is running and using port 11434:\n",
    "   ```bash\n",
    "   netstat -ano | findstr :11434\n",
    "   ```\n",
    "\n",
    "2. **Identify and Kill Background Task (if needed)**:\n",
    "   If you see that another task is using the port, you can identify and kill it:\n",
    "   ```bash\n",
    "   tasklist /FI \"PID eq <PID>\"\n",
    "   taskkill /PID <PID> /F\n",
    "   ```\n",
    "\n",
    "3. **Close Ollama from Windows Task Bar (Optional)**:\n",
    "   If the server is still not responding or you see issues, you can use the **up arrow key** (^) on the Windows task bar (bottom-left) to close any running Ollama instances.\n",
    "\n",
    "4. **Start Jupyter Notebook**:\n",
    "   Once the server is running, start your Jupyter notebook with the necessary code to interact with Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from PyPDF2 import PdfReader\n",
    "from IPython.display import display, Markdown ,update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''You are an agent which analyses the resume and understands the context of resume and \n",
    "    extract the following fields and format the output:\n",
    "    - Name\n",
    "    - Email\n",
    "    - Phone 1\n",
    "    - Phone 2\n",
    "    - Address\n",
    "    - City\n",
    "    - LinkedIn\n",
    "    - Professional Experience (in years)\n",
    "    - Highest Education\n",
    "    - Is Fresher (yes/no)\n",
    "    - Is Student (yes/no)\n",
    "    - Skills (comma-separated)\n",
    "    - Applied For Profile\n",
    "    - Education (Institute Name, Year of Passing, Score)\n",
    "    - Projects (Project Name, Description)\n",
    "    - Professional Experience (Organisation Name, Duration, Profile)\n",
    "\n",
    "    Ensure to format the output as:\n",
    "    Name: [Value]\n",
    "    Email: [Value]\n",
    "    Phone 1: [Value]\n",
    "    ... and provide this summary of resume in markdown format'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as file:\n",
    "            reader = PdfReader(file)\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text()  # Extract text from each page\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from PDF: {e}\")\n",
    "    return text\n",
    "    \n",
    "user_prompt = extract_text_from_pdf('./data/python-developer-resume-example.pdf') + \"Provide a summary of this resume\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def call_ollama(system_prompt, user_prompt):\n",
    "    url = \"http://localhost:11434/api/chat\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": \"llama3.2\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_prompt\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, json=payload)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        full_response = \"\"\n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                json_response = line.decode('utf-8')\n",
    "                try:\n",
    "                    response_data = json.loads(json_response)\n",
    "                    if 'message' in response_data:\n",
    "                        full_response += response_data['message'].get('content', '')\n",
    "                except json.JSONDecodeError:\n",
    "                    print(\"Error decoding JSON\")\n",
    "        \n",
    "        return display(Markdown(full_response))\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "response = call_ollama(system_prompt, user_prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **OLD CODE**\n",
    "\n",
    "**NOTE :** Just Kept the old code to show how Ollama provdes answers it sends one word at a time in a chunk type response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:11434/api/chat\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Define the conversation prompt and specify the model\n",
    "data = {\n",
    "    \"model\": \"llama3.2\",  # Specify the model name here\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Send the POST request to the Ollama server\n",
    "response = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "# Print the raw response text for debugging\n",
    "print(\"Raw Response:\", response.text)\n",
    "\n",
    "# Try to parse the response only if it's in JSON format\n",
    "try:\n",
    "    response_json = response.json()  # Try parsing the response as JSON\n",
    "    print(\"Model Response:\", response_json)\n",
    "except ValueError as e:\n",
    "    print(\"Error parsing JSON:\", e)\n",
    "    print(\"Response text may not be valid JSON.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Easier Way**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Led Zeppelin, often shortened to Led Zeppelin or simply Zep, was a legendary British rock band formed in London in 1968. The band is widely regarded as one of the most influential and iconic rock bands in history.\n",
       "\n",
       "Here's a brief overview:\n",
       "\n",
       "**The Band:**\n",
       "\n",
       "* Robert Plant (vocals)\n",
       "* Jimmy Page (guitar, production)\n",
       "* John Paul Jones (bass guitar, keyboards, production)\n",
       "* John Bonham (drums)\n",
       "\n",
       "**Music Style:**\n",
       "Led Zeppelin was known for their unique blend of blues, folk, and hard rock music. They were heavily influenced by American blues musicians like Robert Johnson and Muddy Waters, as well as classical composers like Richard Wagner.\n",
       "\n",
       "**Notable Songs:**\n",
       "\n",
       "* \"Stairway to Heaven\"\n",
       "* \"Whole Lotta Love\"\n",
       "* \"Kashmir\"\n",
       "* \"Black Dog\"\n",
       "* \"Rock and Roll\"\n",
       "\n",
       "**Impact:**\n",
       "Led Zeppelin's music had a profound impact on the development of rock music. They helped shape the genre, pushing boundaries with their innovative use of guitar, bass, and drums. Their powerful live performances and iconic music videos (for their time) helped establish them as one of the most influential bands in history.\n",
       "\n",
       "**Legacy:**\n",
       "Led Zeppelin's legacy extends far beyond their music. The band has inspired countless other artists, including rock, metal, and hard rock musicians. They've also influenced numerous films, TV shows, and even fashion trends (e.g., Robert Plant's iconic outfits).\n",
       "\n",
       "Despite the tragic passing of John Bonham in 1980 and Jimmy Page's departure from the band, Led Zeppelin continues to be celebrated by fans worldwide.\n",
       "\n",
       "Are you a fan of Led Zeppelin? Do you have a favorite song or album?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "def call_ollama(system_prompt, user_prompt):\n",
    "    stream = ollama.chat(model='llama3.2', messages=[\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': system_prompt\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': user_prompt\n",
    "        }\n",
    "    ])\n",
    "\n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in stream['message']['content']:\n",
    "        response += chunk or''\n",
    "        response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "        update_display(Markdown(response), display_id=display_handle.display_id)\n",
    "        \n",
    "# # Example usage\n",
    "system_prompt = \"You are a helpful assistant who explains complex topics in simple terms.\"\n",
    "user_prompt = \"Who is Led Zepplin\"\n",
    "\n",
    "response = call_ollama(system_prompt, user_prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
