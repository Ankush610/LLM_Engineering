{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Running Jupyter Code with Ollama Server**\n",
    "\n",
    "1. **Download and Install Ollama**: Ensure that Ollama is downloaded and installed on your system.\n",
    "\n",
    "2. **Check Ollama Version**:\n",
    "   ```bash\n",
    "   ollama --version\n",
    "   ```\n",
    "\n",
    "3. **Run Llama 3.2 Model**:\n",
    "   To load the Llama 3.2 model:\n",
    "   ```bash\n",
    "   ollama run llama3.2\n",
    "   ```\n",
    "\n",
    "4. **Start Ollama Server**:\n",
    "   To start the Ollama server, run:\n",
    "   ```bash\n",
    "   ollama serve\n",
    "   ```\n",
    "\n",
    "**Note**: If you face any issues while running the server, checking the background tasks using the port can help resolve conflicts. Always ensure the port is free before starting the server.\n",
    "\n",
    "1. **Check if Ollama Server is Running**:\n",
    "   Check if the server is running and using port 11434:\n",
    "   ```bash\n",
    "   netstat -ano | findstr :11434\n",
    "   ```\n",
    "\n",
    "2. **Identify and Kill Background Task (if needed)**:\n",
    "   If you see that another task is using the port, you can identify and kill it:\n",
    "   ```bash\n",
    "   tasklist /FI \"PID eq <PID>\"\n",
    "   taskkill /PID <PID> /F\n",
    "   ```\n",
    "\n",
    "3. **Close Ollama from Windows Task Bar (Optional)**:\n",
    "   If the server is still not responding or you see issues, you can use the **up arrow key** (^) on the Windows task bar (bottom-left) to close any running Ollama instances.\n",
    "\n",
    "4. **Start Jupyter Notebook**:\n",
    "   Once the server is running, start your Jupyter notebook with the necessary code to interact with Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from PyPDF2 import PdfReader\n",
    "from IPython.display import display, Markdown ,update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''You are an agent which analyses the resume and understands the context of resume and \n",
    "    extract the following fields and format the output:\n",
    "    - Name\n",
    "    - Email\n",
    "    - Phone 1\n",
    "    - Phone 2\n",
    "    - Address\n",
    "    - City\n",
    "    - LinkedIn\n",
    "    - Professional Experience (in years)\n",
    "    - Highest Education\n",
    "    - Is Fresher (yes/no)\n",
    "    - Is Student (yes/no)\n",
    "    - Skills (comma-separated)\n",
    "    - Applied For Profile\n",
    "    - Education (Institute Name, Year of Passing, Score)\n",
    "    - Projects (Project Name, Description)\n",
    "    - Professional Experience (Organisation Name, Duration, Profile)\n",
    "\n",
    "    Ensure to format the output as:\n",
    "    Name: [Value]\n",
    "    Email: [Value]\n",
    "    Phone 1: [Value]\n",
    "    ... and provide this summary of resume in markdown format'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as file:\n",
    "            reader = PdfReader(file)\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text()  # Extract text from each page\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from PDF: {e}\")\n",
    "    return text\n",
    "    \n",
    "user_prompt = extract_text_from_pdf('./data/python-developer-resume-example.pdf') + \"Provide a summary of this resume\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Complete Response by Ollama**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='llama3.2' created_at='2025-01-16T15:52:03.1486778Z' done=True done_reason='stop' total_duration=662937700 load_duration=28479100 prompt_eval_count=36 prompt_eval_duration=81336000 eval_count=9 eval_duration=549193000 message=Message(role='assistant', content='The capital of India is New Delhi.', images=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "def call_ollama(system_prompt, user_prompt):\n",
    "    content = ollama.chat(model='llama3.2', messages=[\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': system_prompt\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': user_prompt\n",
    "        }]\n",
    "        )\n",
    "    \n",
    "    print(content)\n",
    "    \n",
    "# # Example usage\n",
    "system_prompt = \"You are a helpful assistant\"\n",
    "user_prompt = \"What is captial of India\"\n",
    "\n",
    "call_ollama(system_prompt, user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Chunk Response by Ollama**\n",
    "\n",
    "**NOTE :** If we use stream=True parameter model gives response in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='llama3.2' created_at='2025-01-16T16:08:15.4735779Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='The', images=None, tool_calls=None)\n",
      "model='llama3.2' created_at='2025-01-16T16:08:15.5452448Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' capital', images=None, tool_calls=None)\n",
      "model='llama3.2' created_at='2025-01-16T16:08:15.6105445Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' of', images=None, tool_calls=None)\n",
      "model='llama3.2' created_at='2025-01-16T16:08:15.6782778Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' India', images=None, tool_calls=None)\n",
      "model='llama3.2' created_at='2025-01-16T16:08:15.768747Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' is', images=None, tool_calls=None)\n",
      "model='llama3.2' created_at='2025-01-16T16:08:15.8349298Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' New', images=None, tool_calls=None)\n",
      "model='llama3.2' created_at='2025-01-16T16:08:15.9123193Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Delhi', images=None, tool_calls=None)\n",
      "model='llama3.2' created_at='2025-01-16T16:08:15.9755466Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', images=None, tool_calls=None)\n",
      "model='llama3.2' created_at='2025-01-16T16:08:16.0677817Z' done=True done_reason='stop' total_duration=1008032300 load_duration=36204700 prompt_eval_count=36 prompt_eval_duration=376564000 eval_count=9 eval_duration=596866000 message=Message(role='assistant', content='', images=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "def call_ollama(system_prompt, user_prompt):\n",
    "    content = ollama.chat(model='llama3.2', messages=[\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': system_prompt\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': user_prompt\n",
    "        }],\n",
    "        stream=True\n",
    "        )\n",
    "    \n",
    "    for chunk in content:\n",
    "        print(chunk)\n",
    "    \n",
    "# # Example usage\n",
    "system_prompt = \"You are a helpful assistant\"\n",
    "user_prompt = \"What is captial of India\"\n",
    "\n",
    "call_ollama(system_prompt, user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Streaming with Markdown**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's a summary of the resume:\n",
       "\n",
       "**Summary:**\n",
       "This is a professional resume for a Python developer with experience working on various projects, including web applications and data pipelines. The candidate has a strong background in computer science, with degrees from University of Chicago and University of Pittsburgh.\n",
       "\n",
       "**Key Skills:**\n",
       "\n",
       "* Programming languages: HTML/CSS, SQL (PostgreSQL, Oracle), JavaScript (Angular), Python (Django)\n",
       "* Frameworks: Angular, Django\n",
       "* Databases: PostgreSQL, Oracle\n",
       "* Cloud platforms: AWS (Redshift, S3)\n",
       "* Version control: Git\n",
       "\n",
       "**Work Experience:**\n",
       "\n",
       "* Current role as a Python Developer at DoorDash, where the candidate has worked on building new features and improving user experience.\n",
       "* Previous internship at Knewton, where they contributed to implementing RESTful APIs in Django and built unit testing infrastructure for a client web application.\n",
       "\n",
       "**Projects:**\n",
       "\n",
       "* Cryptocurrency Price Tracker, which incorporated API calls to several applications and utilized D3.js for dynamic visualization.\n",
       "\n",
       "**Education:**\n",
       "\n",
       "* M.S. Computer Science, University of Chicago (2014-2016)\n",
       "* B.S. Computer Science, University of Pittsburgh (2010-2014)\n",
       "\n",
       "Overall, this resume showcases the candidate's technical skills, experience working on various projects, and their ability to improve user experience and contribute to data-driven decision making."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "def call_ollama(system_prompt, user_prompt):\n",
    "    stream = ollama.chat(model='llama3.2', messages=[\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': system_prompt\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': user_prompt\n",
    "        }],\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in stream:\n",
    "        response += chunk['message']['content']\n",
    "        response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "        update_display(Markdown(response),display_id=display_handle.display_id)\n",
    "\n",
    "# Example usage\n",
    "system_prompt = \"You are a helpful assistant\"\n",
    "# user_prompt = \"What is the capital of India?\"\n",
    "\n",
    "call_ollama(system_prompt, user_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
